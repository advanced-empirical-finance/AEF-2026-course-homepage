[
  {
    "objectID": "slides-index.html",
    "href": "slides-index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Week\nTopic\nPreparation and reading\nExercise\n\n\n\n\n7\nKick-off\nComplete all technical prerequisites before the lecture\n0, 1, 2\n\n\n8\nOptimal Portfolio Choice\nChapter 3 in Data Science with R (data transformation). Sections 2.1 and 3.1 of Portfolio Choice Problems. Read introduction and Section 1 of Risk Reduction in Large Portfolios\n2\n\n\n9\nCRSP / beta estimation\nChapters 7 and 8 of Empirical Asset Pricing: The Cross-Section of Stock Returns\n3\n\n\n10\nPortfolio sorts\n\n4\n\n\n11\nMachine Learning in Finance\nEmpirical asset pricing via Machine Learning. FT article: “The hidden ‘replication crisis’ of finance”\n5\n\n\n12\nMachine Learning in Finance\n\n6\n\n\n13\nMachine Learning in Finance\n\n7\n\n\n15\nVolatility estimation\n\n8\n\n\n16\nVolatility estimation\n\n9\n\n\n17\nPortfolio optimization\n\n10\n\n\n18\nParametric Portfolio Choice\n\n11\n\n\n19\nHigh-frequency data\n\n12\n\n\n20\nHigh-frequency econometrics\n\n13\n\n\n21\nQ&A\n\n\n\n\n\n\nImportant dates\n\n\n\nSubmission deadline MA 1\nMarch 7 (10am)\n\n\nSubmission deadline MA 2\nApril 11 (10am)\n\n\nSubmission deadline MA 3\nMay 9 (10am)\n\n\nFinal exam\nJune 24 - June 26"
  },
  {
    "objectID": "exercises/technical-prerequisites.html",
    "href": "exercises/technical-prerequisites.html",
    "title": "Technical prerequisites",
    "section": "",
    "text": "Before we start working with R, Python, and Quarto in this course, please make sure you have installed all the required software. Follow the steps below to set up your working environment. Should you encounter any issues, please do not hesitate to contact your TAs or me for assistance. To facilitate collaboration, please ask me via the Absalon discussion board to make sure everyone can benefit from the solution.\nYou can install new packages, e.g., by calling install.packages(\"tidymodels\")´. To collaborate with your peers, just runrenv::snapshot()to keep track of all the packages you use, and share therenv.lockfile. Your colleagues only have to runrenv::restore()` to replicate your exact R package environment.",
    "crumbs": [
      "exercises",
      "Introduction",
      "Technical prerequisites"
    ]
  },
  {
    "objectID": "exercises/technical-prerequisites.html#optional-prerequisites",
    "href": "exercises/technical-prerequisites.html#optional-prerequisites",
    "title": "Technical prerequisites",
    "section": "Optional prerequisites",
    "text": "Optional prerequisites\n\nInstall the newest version of Quarto. (Quarto comes prebundled with Positron, so you will have access to quarto once you followed the previous step)\nIn order to create PDFs you will need to install a recent distribution of LaTeX. I recomment to install TinyTeX, a lightweight, cross-platform, portable, and easy-to-maintain LaTeX distribution by typing the command\n   quarto install tool tinytex\n\nin Positron / Terminal or directly in the console",
    "crumbs": [
      "exercises",
      "Introduction",
      "Technical prerequisites"
    ]
  },
  {
    "objectID": "exercises/portfolio_sorts.html",
    "href": "exercises/portfolio_sorts.html",
    "title": "Portfolio sorts",
    "section": "",
    "text": "Exercises:\n\nLoad the monthly CRSP file, the Fama-French Factors, and the estimated betas from the tidy_finance_*.sqlite database.\nCreate portfolio sorts based on the lagged beta. Specifically, you compute the breakpoint as the median lag beta each month. Then, you compute the returns of a portfolio that invests only in the stocks with a higher beta than the breakpoint and a portfolio that invests only in the stocks with a lower beta than the breakpoints. The portfolio weights can either be equal or value-weighted.\nWhat are the monthly excess returns of both portfolios?\nDoes a portfolio that goes long high beta stocks and short low beta stocks yield an excess return significantly different from zero?\nWrite a general function for portfolio sorts based on a variable number of breakpoints. Then, compute portfolio returns based on lagged beta decile sorts.\nWhat is the CAPM alpha of the ten portfolio returns? Is this finding in line with your expectations based on the CAPM implications?\nDoes a high beta minus low beta portfolio yield abnormal excess returns?\n\nSolutions: All solutions are provided in the book chapter Univariate portfolio sorts (R Version) or Univariate portfolio sorts (Python Version)",
    "crumbs": [
      "exercises",
      "Asset Pricing",
      "Portfolio sorts"
    ]
  },
  {
    "objectID": "exercises/machine_learning_nonlinear.html",
    "href": "exercises/machine_learning_nonlinear.html",
    "title": "Neural networks",
    "section": "",
    "text": "Machine Learning 2: Random Forests and (deep) neural networks\nIn this exercise set you familiarize yourself with the inner working of tidymodels (scikit-learn for Python) and torch for deep neural networks.\nYou will apply well-known methods such as random forests and neural networks to a simple application in option pricing. More specifically, we are going to create an artificial dataset of option prices for different values based on the Black-Scholes pricing equation for call options. Then, we train different models to learn how to price call options without prior knowledge of the theoretical underpinnings of the famous option pricing equation.\nExercises: \n\nSimulate prices for Call options based on a grid of different combinations of times to maturity (T), risk-free rates (r), volatilities (sigma), strike prices (K), and current stock prices (S). Compute the Black-Scholes option price for each combination of your grid.\nAdd an idiosyncratic error term to each observation such that the prices considered do not exactly reflect the values implied by the Black-Scholes equation.\nSplit the data into a training set (which contains some of the observed option prices) and a test set that will only be used for the final evaluation\nCreate a recipe which normalizes all predictors to have zero mean and unit standard deviation.\nFit a neural network with one hidden layer on the training data.\nFit a random forest (rand_forest) on the training data.\nFigure out how you create a deep neural network which predicts option prices from the set of input parameters. Figure out how to set the activation function, the number of neurons per layer and compile the model.\nFit the deep neural network with the pre-processed training data\nEvaluate the predictive performance of your competing model. Which model performed best in the test sample to predict option prices?\n\nSolutions:  All solutions are provided in the book chapter Option pricing via machine learning (R version) or Option pricing via machine learning (Python version)",
    "crumbs": [
      "exercises",
      "Machine Learning",
      "Neural networks"
    ]
  },
  {
    "objectID": "exercises/introduction.html",
    "href": "exercises/introduction.html",
    "title": "Introduction to Tidy Finance",
    "section": "",
    "text": "The main aim of this chapter is to familiarize yourself with tidy coding principles. We start by downloading and visualizing stock data before we move to a simple portfolio choice problem. These examples introduce you to our approach of Tidy Finance.",
    "crumbs": [
      "exercises",
      "Introduction",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "exercises/introduction.html#working-with-stock-market-data",
    "href": "exercises/introduction.html#working-with-stock-market-data",
    "title": "Introduction to Tidy Finance",
    "section": "Working with stock market data",
    "text": "Working with stock market data\nExercises:\n\nDownload daily prices for one stock market ticker of your choice (e.g. AAPL) from Yahoo!Finance. Plot the time series of adjusted closing prices. To download the data you can use the command download_data from the tidyfinance package (R) or the tidyfinance library (Python). If you do not know how to use the functionalities, make sure you read the help file by calling ?download_data.\nCompute daily net returns for the asset and visualize the distribution of daily returns in a histogram using ggplot2 (R) or plotnine (Python). Also, use geom_vline() to add a dashed red line that indicates the 5% quantile of the daily returns within the histogram.\nCompute summary statistics (mean, standard deviation, minimum and maximum) for the daily returns.\n\nSolutions: All tasks are solved and described in detail in Chapter 1.1 in Tidy Finance with R. For a Python version of the code, consult Chapter 1.1 in Tidy Finance with Python",
    "crumbs": [
      "exercises",
      "Introduction",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "exercises/introduction.html#scaling-up-the-analysis",
    "href": "exercises/introduction.html#scaling-up-the-analysis",
    "title": "Introduction to Tidy Finance",
    "section": "Scaling up the analysis",
    "text": "Scaling up the analysis\nAs a next step, we generalize the code from before such that all the computations can handle an arbitrary vector of tickers (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\nExercises:\n\nNow the tidyverse magic starts: Take your code from before and generalize it such all the computations are performed for an arbitrary vector of tickers (e.g., ticker &lt;- c(\"AAPL\", \"MMM\", \"BA\") (R) or ticker = [\"AAPL\", \"MMM\", \"BA\"] (Python)). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\nConsider the research question Are days with high aggregate trading volume often followed by high aggregate trading volume days? How would you investigate this question? Hint: Compute the aggregate daily trading volume (in USD) and find an appropriate visualization to analyze the question.\nAsk yourself how the visualization could be improved by adding further information (e.g., highlighting days with extreme trading volume), improving axis labels, changing the style (colors, themes, etc.). Implement your ideas and create a final version of the plot. Make sure the plot is self-explanatory (e.g., contains captions, legends, an appropriate title, etc.). Upload your final plot as a .jpg file to the Absalon discussion board.\n\nSolutions: All tasks are solved and described in detail in Tidy Finance with R in the Chapters Scaling up the analysis and Other forms of data aggregation and in Tidy with Python in the Chapters Scaling up the analysis and Other forms of data aggregation.",
    "crumbs": [
      "exercises",
      "Introduction",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "exercises/introduction.html#portfolio-choice-problems",
    "href": "exercises/introduction.html#portfolio-choice-problems",
    "title": "Introduction to Tidy Finance",
    "section": "Portfolio choice problems",
    "text": "Portfolio choice problems\nExercises:\n\nCompute monthly returns from all adjusted daily Dow Jones constituent prices. Hint: You can download the constituents of the Dow Jones 30 index from this homepage.\nCompute the vector of historical average returns and the sample variance-covariance matrix.\nCompute the minimum variance portfolio weights and the portfolio volatility and average returns.\nVisualize the mean-variance efficient frontier. For that, first, compute the efficient portfolio for an arbitrary risk-free rate (below the average return of the minimum variance portfolio). Then use the two-fund separation theorem to compute the location of a grid of combinations of the minimum variance portfolio and the efficient portfolio on the mean-volatility diagram which has (annualized) volatility on the \\(x\\) axis and (annualized) expected returns on the \\(y\\) axis. Indicate the location of the individual assets on the mean-volatility diagram as well.\n\nSolutions: All tasks are solved and described in detail in Tidy Finance with R and Tidy Finance with Python, in the Chapters Portfolio Choice Problems and The Efficient Frontier.",
    "crumbs": [
      "exercises",
      "Introduction",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "exercises/introduction.html#equivalence-between-certainty-equivalent-maximization-and-minimum-variance-optimization",
    "href": "exercises/introduction.html#equivalence-between-certainty-equivalent-maximization-and-minimum-variance-optimization",
    "title": "Introduction to Tidy Finance",
    "section": "Equivalence between Certainty equivalent maximization and minimum variance optimization",
    "text": "Equivalence between Certainty equivalent maximization and minimum variance optimization\nIn the lecture slides (Parameter uncertainty), I argue that an investor with a quadratic utility function with a certainty equivalent \\[\\max_w CE(w) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen to minimize volatility given a pre-specified level or expected returns \\[\n\\min_w \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1\n\\]\nNote the differences: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines her optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return she wants to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk-averse investors (higher \\(\\gamma\\)) will choose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nExercises:  Proof that the optimal portfolio weights are equivalent in both cases.\nSolutions: The proofs are provided in the Appendix of Tidy Finance with R.",
    "crumbs": [
      "exercises",
      "Introduction",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "exercises/gu_kelly_xiu.html",
    "href": "exercises/gu_kelly_xiu.html",
    "title": "Empirical Asset Pricing via Machine Learning",
    "section": "",
    "text": "This exercise should help you to fight your way through an actual academic application of machine learning methods in asset pricing. The exercise guides you step-by-step to replicate the empirical framework and, therefore, at some point also the main results of the paper Empirical Asset Pricing via Machine Learning by Shihao Gu, Bryan Kelly, and Dacheng Xiu.\nExercises:\n\nStart reading the paper from Section 2 (“An empirical study of U.S. equities”). In their study, the authors aim to use different machine learning procedures to approximate the overarching empirical model \\(E_t\\left(r_{i, t+1}\\right) = g^\\star(z_{i,t})\\) as defined in the paper. The returns are monthly total individual equity returns from CRSP for all firms listed in the NYSE, AMEX, and NASDAQ. The sample starts in March 1957 (the start date of the S&P 500) and ends in December 2016, totaling 60 years. Prepare the data from CRSP according to these requirements.\nOne significant contribution of the paper is implementing the predictive framework with a vast collection of stock-level predictive characteristics based on the cross-section of stock returns literature. Table A.6 in the Internet Appendix details all these characteristics. You do not have to generate each of the characteristics. Instead download the data from Dacheng Xiu’s homepage.\nRead the readme file and process the data the way the authors explain it in footnote 29 of the paper: “We cross-sectionally rank all stock characteristics period-by-period and map these ranks into the [-1,1] interval following Kelly, Pruitt, and Su (2019) and Freyberger, Neuhierl, and Weber (2020).”\nMerge the dataset with the CRSP file and the macroeconomic predictors following the variable definitions detailed in Welch and Goyal (2008). (Hint: the macroeconomic variables are part of the tidy_finance_*.sqlite database, consult Chapter 2 for more information).\nReplace missing values with the cross-sectional median at each month for each stock\nThen, process the column `sic2’, which currently contains categorical data and which should be transformed into a matrix with 74 columns: The values in the cells should be one if the ‘permno’ corresponds to the specific industry classification and 0 otherwise\nIn the original paper, the authors inflate the dataset by considering any possible interaction between macroeconomic variables and firm characteristics. How could you implement this? (Note: the dataset is enormous already without interaction terms. For this lecture, feel free to skip this final step.)\n\nSolutions\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(RSQLite)\nlibrary(lubridate)\nlibrary(archive)\n\n\n\n\nimport pandas as pd\nimport requests\nimport numpy as np\nfrom io import BytesIO, StringIO\nimport zipfile\nimport sqlite3\n\n\n\n\n\nRPython\n\n\nFirst, we download the (large) dataset directly from within R. The data is stored as a .csv file within a zipped folder. For such purposes, the archive package is useful. Presumably, you want to store the output in a .sqlite file, for instance, the file tidy_finance_*.sqlite or in a new file. Next, we set options(timeout = 200), which allows the R session to download the dataset for 200 seconds. The default in R is 60 seconds which was too short on my machine.\n\noptions(timeout = 1200)\ncharacteristics &lt;- read_csv(archive_read(\"https://dachxiu.chicagobooth.edu/download/datashare.zip\", \n                                         file = \"datashare.csv\"))\n\ncharacteristics &lt;- characteristics |&gt;\n  rename(\"month\" = \"DATE\") |&gt;\n  mutate(\n    month = ymd(month),\n    month = floor_date(month, \"month\")\n  ) |&gt;\n  rename_with(~ paste0(\"characteristic_\", .), -c(permno, month, sic2))\n\n\n\nFirst, we download the (large) dataset directly from within Python. The data is stored as a .csv file within a zipped folder. For such purposes, the requests library is useful. Presumably, you want to store the output in a .sqlite file, for instance, the file tidy_finance_*.sqlite or in a new file. We set timeout = 1200, which allows the session to download the dataset for 1200 seconds. The default was too short on my machine.\n\nurl = \"https://dachxiu.chicagobooth.edu/download/datashare.zip\"\nresponse = requests.get(url, timeout=1200, verify=False)\n\n# Extract the contents of the zip file\nwith zipfile.ZipFile(BytesIO(response.content)) as z:\n    csv_file = z.open(z.namelist()[0])\n    # Read the CSV file\n    characteristics = pd.read_csv(csv_file)\n\ncharacteristics = (characteristics\n    .rename(columns={'DATE': 'month'})\n    .assign(month=lambda df: pd.to_datetime(df['month']).dt.to_period('M').dt.to_timestamp())\n    .rename(columns={col: 'characteristic_' + col for col in characteristics.columns if col not in ['permno', 'month', 'sic2']})\n)    \n\n\n\n\nThe cross-sectional ranking may be time-consuming. The idea is that at each date, the cross-section of each predictor should be scaled such that the maximum value is one and the minimum value is -1. The function below explicitly handles NA values, so they do not tamper with the ranking.\n\nRPython\n\n\n\nrank_transform &lt;- function(x) {\n  rx &lt;- rank(x, na.last = TRUE)\n  non_nas &lt;- sum(!is.na(x))\n  rx[rx &gt; non_nas] &lt;- NA\n  2 * ((rx - min(rx, na.rm = TRUE)) / (max(rx, na.rm = TRUE) - min(rx, na.rm = TRUE)) - 0.5)\n}\n\ncharacteristics &lt;- characteristics |&gt;\n  group_by(month) |&gt;\n  mutate(across(contains(\"characteristic\"), rank_transform))\n\n\n\n\ndef rank_transform(x):\n    rx = x.rank(na_option='keep')\n    non_nas = x.count()\n    rx[rx &gt; non_nas] = np.nan\n    return 2 * ((rx - rx.min()) / (rx.max() - rx.min()) - 0.5)\n\n\ncharacteristics = (characteristics\n    .groupby('month', group_keys=False)\n    .apply(lambda group: group.apply(lambda x: rank_transform(x) if 'characteristic' in x.name and x.name != 'month' else x))\n)\n\n\n\n\nNext, merge the data with monthly CRSP data to get excess returns. To create portfolio sorts based on the predictions, mktcap_lag remains in the sample. As a final step, we also include the macroeconomic predictor variables.\n\nRPython\n\n\n\ntidy_finance &lt;- dbConnect(SQLite(), \"../data/tidy_finance.sqlite\",\n  extended_types = TRUE\n)\n\ncrsp_monthly &lt;- tbl(tidy_finance, \"crsp_monthly\") |&gt;\n  select(month, permno, mktcap_lag, ret_excess) |&gt;\n  collect()\n\nmacro_predictors &lt;- tbl(tidy_finance, \"macro_predictors\") |&gt;\n  select(-rp_div) |&gt;\n  collect() |&gt;\n  rename_with(~ paste0(\"macro_\", .), -month)\n\ncharacteristics &lt;- characteristics |&gt;\n  inner_join(crsp_monthly, by = c(\"month\", \"permno\")) |&gt;\n  inner_join(macro_predictors, by = \"month\") |&gt;\n  arrange(permno, month) |&gt;\n  select(permno, month, ret_excess, mktcap_lag, sic2, contains(\"macro\"), contains(\"characteristic\"))\n\n\n\n\ntidy_finance = sqlite3.connect(database=\"../data/tidy_finance_python.sqlite\")\n\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT month, permno, mktcap_lag, ret_excess FROM crsp_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\nmacro_predictors = (pd.read_sql_query(\n    sql=\"SELECT * FROM macro_predictors\",\n    con=tidy_finance\n).drop(columns=['rp_div'])\n .rename(columns=lambda x: f'macro_{x}' if x != 'month' else x)\n)\n\ncharacteristics = (\n    characteristics\n    .merge(crsp_monthly, on=['month', 'permno'], how='inner')\n    .merge(macro_predictors, on='month', how='inner')\n    .sort_values(by=['permno', 'month'])\n    .reset_index(drop=True)  \n    [['permno', 'month', 'ret_excess', 'mktcap_lag', 'sic2'] +\n     [col for col in characteristics.columns if 'macro_' in col] +\n     [col for col in characteristics.columns if 'characteristic_' in col]]\n)\n\n\n\n\nThe code below replaces missing values with each stock’s cross-sectional median at each month. One of the paper’s coauthors also claims that NA values are set to zero.\n\nRPython\n\n\n\nreplace_nas &lt;- function(x) {\n  x[is.na(x)] &lt;- median(x, na.rm = TRUE)\n  return(x)\n}\n\ncharacteristics &lt;- characteristics |&gt;\n  group_by(month) |&gt;\n  mutate(across(contains(\"characteristic\"), replace_nas)) |&gt;\n  drop_na(sic2) |&gt;\n  mutate(sic2 = as_factor(sic2))\n\n\n\n\ndef replace_nas(x):\n    x[x.isna()] = x.median(skipna=True)\n    return x\n\ncharacteristics = (characteristics\n    .groupby('month')\n    .apply(lambda group: group.apply(replace_nas) if group.select_dtypes(include='number').any().any() else group)\n    .reset_index(drop=True)\n    .dropna(subset=['sic2'])\n    .astype('category')\n)\n\n\n\n\nFinally, for your convenience, I include the pre-processed file into a new file, the tidy_finance_ML.sqlite database. You find the document on Absalon and can use it for your research and replication attempts.\n\nRPython\n\n\n\ntidy_finance_ML &lt;- dbConnect(SQLite(), \"../data/tidy_finance_ML.sqlite\",\n  extended_types = TRUE\n)\n\ndbWriteTable(tidy_finance_ML,\n  \"stock_characteristics_monthly\",\n  value = characteristics |&gt; ungroup(),\n  overwrite = TRUE\n)\n\n\n\n\ntidy_finance_ML = sqlite3.connect(database=\"../data/tidy_finance_ML.sqlite\")\n\n\ncharacteristics.to_sql(\n  name=\"stock_characteristics_monthly\",\n  con=tidy_finance_ML, \n  if_exists=\"replace\", \n  index=False\n  )",
    "crumbs": [
      "exercises",
      "Machine Learning",
      "Empirical Asset Pricing via Machine Learning"
    ]
  },
  {
    "objectID": "exercises/covariance_estimation.html",
    "href": "exercises/covariance_estimation.html",
    "title": "(Co)variance estimation",
    "section": "",
    "text": "This short exercise illustrates how to perform maximum likelihood estimation using the simple example of ARCH\\((p)\\) and GARCH(\\(p, q\\)) models. First, write the code for the basic specification independently. Afterward, the exercises will help you familiarize yourself with well-established packages that provide the same (and much more sophisticated) methods to estimate conditional volatility models.\nExercises:\n\nAs a benchmark dataset, download prices and compute daily returns for all stocks that are part of the Dow Jones 30 index (ticker &lt;- \"DOW\"). The sample period should start on January 1st, 2000. Then, decompose the time series into a predictable part and the residual, \\(r_t = E(r_t|\\mathcal{F}_{t-1}) + \\varepsilon_t\\). The most straightforward approach would be to demean the time series by simply subtracting the sample mean, but in principle, more sophisticated methods can be used.\nFor each of the 30 tickers, illustrate the rolling window standard deviation based on some suitable estimation window length.\nWrite a small function that computes the maximum likelihood estimator (MLE) of an ARCH(\\(p\\)) model.\nWrite a second function implementing MLE estimation for a GARCH(\\(p,q\\)) model.\nWhat is the unconditional estimated variance of the ARCH(\\(1\\)) and the GARCH(\\(1, 1\\)) model for each ticker?\n\n\n\nCompute and illustrate the model-implied Value-at-risk, defined as the lowest return your model expects with a probability of less than 5 %. Formally, the VaR is defined as \\(\\text{VaR} _{\\alpha }(X)= -\\inf {\\big \\{}x\\in \\mathbb {R} :F_{-X}(x)&gt;\\alpha {\\big \\}}=F_{-X}^{-1}(1-\\alpha)\\) where \\(X\\) is the return distribution. Illustrate stock returns that fall below the estimated Value-at-risk. What can you conclude?\n\nSolutions:\n\nlibrary(tidyverse)\nlibrary(tidyquant)\n\nAs usual, I use the tidyquant package to download price data of the time series of 30 stocks and to compute (adjusted) net returns. Then, I demean the return time series. This step could be replaced by either adjusting return based on some estimated factor model or predictions that stem from the part on Machine Learning.\n\nticker &lt;- tq_index(\"DOW\") \nindex_prices &lt;- tq_get(ticker,\n  get = \"stock.prices\",\n  from = \"2000-01-01\"\n)\n\nreturns &lt;- index_prices |&gt;\n  group_by(symbol) |&gt;\n  mutate(ret = 100 * (adjusted / lag(adjusted) - 1)) |&gt;\n  select(symbol, date, ret) |&gt;\n  drop_na(ret) |&gt;\n  mutate(ret = ret - mean(ret))\n\nSimilar to the Chapter on beta estimation, I use the slider package for convenient rolling window computations. For detailed documentation, consult (this homepage.)[https://github.com/DavisVaughan/slider]. The option `.complete = TRUE’ ensures that the rolling standard deviations are only computed if sufficient data is available.\n\nlibrary(slider)\n\nrolling_sd &lt;- returns |&gt; \n  group_by(symbol) |&gt; \n  mutate(rolling_sd = slide_dbl(ret, sd, \n                                .before = 100,\n                                .complete = TRUE)) # 100 day estimation window\n\nrolling_sd |&gt;\n  drop_na() |&gt; \n  ggplot(aes(x=date, \n             y = sqrt(250) * rolling_sd, \n             color = symbol)) + \n  geom_line() +\n  labs(x = \"\", y = \"Standard deviation (annualized)\") +\n  theme(legend.position = \"None\")\n\nThe figure illustrates at least two relevant features: i) Although rather persistent, volatilities change over time, and ii) volatilities tend to co-move. Next, I write a simplified script that performs maximum likelihood estimation of the parameters of an ARCH(\\(p\\)) model for Gaussian distributed innovations.\n\nlogL_arch &lt;- function(params, p, ret){\n  # Inputs: params (a p + 1 x 1 vector of ARCH parameters)\n  #         p (lag-length)\n  #         ret demeaned returns \n  # Output: the log-likelihood (for normally distributed innovation terms)\n  eps_sqr_lagged &lt;- cbind(ret, 1)\n  for(i in 1:p){\n    eps_sqr_lagged &lt;- cbind(eps_sqr_lagged, lag(ret, i)^2)\n  }\n  eps_sqr_lagged &lt;- na.omit(eps_sqr_lagged)\n  sigma &lt;- eps_sqr_lagged[,-1] %*% params\n  log_likelihood &lt;- 0.5 * sum(log(sigma)) + 0.5 * sum(eps_sqr_lagged[,1]^2/sigma)\n  return(log_likelihood)\n}\n\nThe function logL_arch computes an ARCH specification’s (log) likelihood with \\(p\\) lags. The function returns the negative log-likelihood because most optimization procedures in R are designed to search for minima instead of maximization.\nThe following lines show how to estimate the model for the time series of demeaned APPL returns (in percent) with optim and p = 2 lags. Note my (almost) arbitrary choice for the initial parameter vector. Also, note that the function logL_arch and the optimization procedure do not enforce the regularity conditions, which would ensure stationary volatility processes.\n\np &lt;- 2\nret &lt;- returns |&gt; filter(symbol == \"AAPL\") |&gt; pull(ret)\n\ninitial_params &lt;- (c(sd(ret), rep(1, p)))\nfit_manual &lt;- optim(par = initial_params, \n                    fn = logL_arch, \n                    hessian = TRUE, \n                    method=\"L-BFGS-B\", \n                    lower = 0, \n                    p = p,\n                    ret = ret)\n\nfitted_params &lt;- (fit_manual$par)\nse &lt;- sqrt(diag(solve(fit_manual$hessian)))\n\ncbind(fitted_params, se) |&gt; knitr::kable(digits = 2)\n\n# Run the code below to compare the results with tseries package\n# library(series)\n# summary(garch(ret,c(0,p)))\n\nWe can implement GARCH estimation with very few adjustments\n\nlogL_garch &lt;- function(params, p, q, ret, \n                       return_only_loglik = TRUE){\n  eps_sqr_lagged &lt;- cbind(ret, 1)\n  for(i in 1:p){\n    eps_sqr_lagged &lt;- cbind(eps_sqr_lagged, lag(ret, i)^2)\n  }\n  sigma.sqrd &lt;- rep(sd(ret)^2, nrow(eps_sqr_lagged))\n  for(t in (1 + max(p, q)):nrow(eps_sqr_lagged)){\n    sigma.sqrd[t] &lt;- params[1:(1+p)]%*% eps_sqr_lagged[t,-1] + \n              params[(2+p):length(params)] %*% sigma.sqrd[(t-1):(t-q)]\n  }\n  sigma.sqrd &lt;- sigma.sqrd[-(1:(max(p, q)))]\n  \n  if(return_only_loglik){\n    0.5 * sum(log(sigma.sqrd)) + 0.5 * sum(eps_sqr_lagged[(1 + max(p, q)):nrow(eps_sqr_lagged),1]^2/sigma.sqrd)\n  }else{\n    return(sigma.sqrd)\n  }\n}\n\np &lt;- 1 # Lag structure \nq &lt;- 1 \nfit_garch_manual &lt;- optim(par = rep(0.01, p + q + 1), \n                    fn = logL_garch, \n                    hessian = TRUE, \n                    method=\"L-BFGS-B\", \n                    lower = 0, \n                    p = p,\n                    q = q,\n                    ret = ret)\n\nfitted_garch_params &lt;- fit_garch_manual$par\n\nNext, I plot the time series of the estimated conditional variances \\(\\sigma_t\\) based on the output. To keep the required code clean and simple, my function logL_garch returns the fitted variances with the option return_only_loglik = FALSE.\n\ntibble(vola = logL_garch(fitted_garch_params, p, q, ret, FALSE)) |&gt; \n  ggplot(aes(x = 1:length(vola), y = sqrt(vola))) +\n  geom_line() +\n  labs(x = \"\", y  = \"GARCH(1,1) volatility\")\n\nNext, the typical tidyverse approach: Once we implement the workflow for one asset, we can use mutate and map() to perform the exact computation in a tidy manner for all assets in our sample. The lecture slides define the unconditional variance.\n\nunconditional_volatility &lt;- returns |&gt; \n  arrange(symbol, date) |&gt; \n  nest(data = c(date, ret)) |&gt; \n  mutate(arch = map(data, function(.x){\n    \n    ret &lt;- .x |&gt; pull(ret) \n    p &lt;- 2\n    initial_params &lt;- (c(sd(ret), rep(1, p)))\n    fit_manual &lt;- optim(par = initial_params, \n                        fn = logL_arch, \n                        hessian = TRUE, \n                        method=\"L-BFGS-B\", \n                        lower = 0, \n                        p = p,\n                        ret = ret)\n    \n    fitted_params &lt;- (fit_manual$par)\n    se &lt;- sqrt(diag(solve(fit_manual$hessian)))\n    \n    return(tibble(param = c(\"intercept\", paste0(\"alpha_\", 1:p)),\n                  value = fitted_params, se = se))\n  })) |&gt; \n  select(-data) |&gt; \n  unnest(arch) |&gt;\n  summarise(uncond_var = dplyr::first(value) / (1 - sum(value) + dplyr::first(value)))\n  \nunconditional_volatility |&gt; \n  ggplot() + \n  geom_bar(aes(x = reorder(symbol, uncond_var), \n               y = sqrt(250) * uncond_var), stat = \"identity\") + \n  coord_flip() +\n  labs(x = \"Unconditional volatility (annualized)\", \n      y = \"\")"
  },
  {
    "objectID": "exercises/covariance_estimation.html#arch-and-garch",
    "href": "exercises/covariance_estimation.html#arch-and-garch",
    "title": "(Co)variance estimation",
    "section": "",
    "text": "This short exercise illustrates how to perform maximum likelihood estimation using the simple example of ARCH\\((p)\\) and GARCH(\\(p, q\\)) models. First, write the code for the basic specification independently. Afterward, the exercises will help you familiarize yourself with well-established packages that provide the same (and much more sophisticated) methods to estimate conditional volatility models.\nExercises:\n\nAs a benchmark dataset, download prices and compute daily returns for all stocks that are part of the Dow Jones 30 index (ticker &lt;- \"DOW\"). The sample period should start on January 1st, 2000. Then, decompose the time series into a predictable part and the residual, \\(r_t = E(r_t|\\mathcal{F}_{t-1}) + \\varepsilon_t\\). The most straightforward approach would be to demean the time series by simply subtracting the sample mean, but in principle, more sophisticated methods can be used.\nFor each of the 30 tickers, illustrate the rolling window standard deviation based on some suitable estimation window length.\nWrite a small function that computes the maximum likelihood estimator (MLE) of an ARCH(\\(p\\)) model.\nWrite a second function implementing MLE estimation for a GARCH(\\(p,q\\)) model.\nWhat is the unconditional estimated variance of the ARCH(\\(1\\)) and the GARCH(\\(1, 1\\)) model for each ticker?\n\n\n\nCompute and illustrate the model-implied Value-at-risk, defined as the lowest return your model expects with a probability of less than 5 %. Formally, the VaR is defined as \\(\\text{VaR} _{\\alpha }(X)= -\\inf {\\big \\{}x\\in \\mathbb {R} :F_{-X}(x)&gt;\\alpha {\\big \\}}=F_{-X}^{-1}(1-\\alpha)\\) where \\(X\\) is the return distribution. Illustrate stock returns that fall below the estimated Value-at-risk. What can you conclude?\n\nSolutions:\n\nlibrary(tidyverse)\nlibrary(tidyquant)\n\nAs usual, I use the tidyquant package to download price data of the time series of 30 stocks and to compute (adjusted) net returns. Then, I demean the return time series. This step could be replaced by either adjusting return based on some estimated factor model or predictions that stem from the part on Machine Learning.\n\nticker &lt;- tq_index(\"DOW\") \nindex_prices &lt;- tq_get(ticker,\n  get = \"stock.prices\",\n  from = \"2000-01-01\"\n)\n\nreturns &lt;- index_prices |&gt;\n  group_by(symbol) |&gt;\n  mutate(ret = 100 * (adjusted / lag(adjusted) - 1)) |&gt;\n  select(symbol, date, ret) |&gt;\n  drop_na(ret) |&gt;\n  mutate(ret = ret - mean(ret))\n\nSimilar to the Chapter on beta estimation, I use the slider package for convenient rolling window computations. For detailed documentation, consult (this homepage.)[https://github.com/DavisVaughan/slider]. The option `.complete = TRUE’ ensures that the rolling standard deviations are only computed if sufficient data is available.\n\nlibrary(slider)\n\nrolling_sd &lt;- returns |&gt; \n  group_by(symbol) |&gt; \n  mutate(rolling_sd = slide_dbl(ret, sd, \n                                .before = 100,\n                                .complete = TRUE)) # 100 day estimation window\n\nrolling_sd |&gt;\n  drop_na() |&gt; \n  ggplot(aes(x=date, \n             y = sqrt(250) * rolling_sd, \n             color = symbol)) + \n  geom_line() +\n  labs(x = \"\", y = \"Standard deviation (annualized)\") +\n  theme(legend.position = \"None\")\n\nThe figure illustrates at least two relevant features: i) Although rather persistent, volatilities change over time, and ii) volatilities tend to co-move. Next, I write a simplified script that performs maximum likelihood estimation of the parameters of an ARCH(\\(p\\)) model for Gaussian distributed innovations.\n\nlogL_arch &lt;- function(params, p, ret){\n  # Inputs: params (a p + 1 x 1 vector of ARCH parameters)\n  #         p (lag-length)\n  #         ret demeaned returns \n  # Output: the log-likelihood (for normally distributed innovation terms)\n  eps_sqr_lagged &lt;- cbind(ret, 1)\n  for(i in 1:p){\n    eps_sqr_lagged &lt;- cbind(eps_sqr_lagged, lag(ret, i)^2)\n  }\n  eps_sqr_lagged &lt;- na.omit(eps_sqr_lagged)\n  sigma &lt;- eps_sqr_lagged[,-1] %*% params\n  log_likelihood &lt;- 0.5 * sum(log(sigma)) + 0.5 * sum(eps_sqr_lagged[,1]^2/sigma)\n  return(log_likelihood)\n}\n\nThe function logL_arch computes an ARCH specification’s (log) likelihood with \\(p\\) lags. The function returns the negative log-likelihood because most optimization procedures in R are designed to search for minima instead of maximization.\nThe following lines show how to estimate the model for the time series of demeaned APPL returns (in percent) with optim and p = 2 lags. Note my (almost) arbitrary choice for the initial parameter vector. Also, note that the function logL_arch and the optimization procedure do not enforce the regularity conditions, which would ensure stationary volatility processes.\n\np &lt;- 2\nret &lt;- returns |&gt; filter(symbol == \"AAPL\") |&gt; pull(ret)\n\ninitial_params &lt;- (c(sd(ret), rep(1, p)))\nfit_manual &lt;- optim(par = initial_params, \n                    fn = logL_arch, \n                    hessian = TRUE, \n                    method=\"L-BFGS-B\", \n                    lower = 0, \n                    p = p,\n                    ret = ret)\n\nfitted_params &lt;- (fit_manual$par)\nse &lt;- sqrt(diag(solve(fit_manual$hessian)))\n\ncbind(fitted_params, se) |&gt; knitr::kable(digits = 2)\n\n# Run the code below to compare the results with tseries package\n# library(series)\n# summary(garch(ret,c(0,p)))\n\nWe can implement GARCH estimation with very few adjustments\n\nlogL_garch &lt;- function(params, p, q, ret, \n                       return_only_loglik = TRUE){\n  eps_sqr_lagged &lt;- cbind(ret, 1)\n  for(i in 1:p){\n    eps_sqr_lagged &lt;- cbind(eps_sqr_lagged, lag(ret, i)^2)\n  }\n  sigma.sqrd &lt;- rep(sd(ret)^2, nrow(eps_sqr_lagged))\n  for(t in (1 + max(p, q)):nrow(eps_sqr_lagged)){\n    sigma.sqrd[t] &lt;- params[1:(1+p)]%*% eps_sqr_lagged[t,-1] + \n              params[(2+p):length(params)] %*% sigma.sqrd[(t-1):(t-q)]\n  }\n  sigma.sqrd &lt;- sigma.sqrd[-(1:(max(p, q)))]\n  \n  if(return_only_loglik){\n    0.5 * sum(log(sigma.sqrd)) + 0.5 * sum(eps_sqr_lagged[(1 + max(p, q)):nrow(eps_sqr_lagged),1]^2/sigma.sqrd)\n  }else{\n    return(sigma.sqrd)\n  }\n}\n\np &lt;- 1 # Lag structure \nq &lt;- 1 \nfit_garch_manual &lt;- optim(par = rep(0.01, p + q + 1), \n                    fn = logL_garch, \n                    hessian = TRUE, \n                    method=\"L-BFGS-B\", \n                    lower = 0, \n                    p = p,\n                    q = q,\n                    ret = ret)\n\nfitted_garch_params &lt;- fit_garch_manual$par\n\nNext, I plot the time series of the estimated conditional variances \\(\\sigma_t\\) based on the output. To keep the required code clean and simple, my function logL_garch returns the fitted variances with the option return_only_loglik = FALSE.\n\ntibble(vola = logL_garch(fitted_garch_params, p, q, ret, FALSE)) |&gt; \n  ggplot(aes(x = 1:length(vola), y = sqrt(vola))) +\n  geom_line() +\n  labs(x = \"\", y  = \"GARCH(1,1) volatility\")\n\nNext, the typical tidyverse approach: Once we implement the workflow for one asset, we can use mutate and map() to perform the exact computation in a tidy manner for all assets in our sample. The lecture slides define the unconditional variance.\n\nunconditional_volatility &lt;- returns |&gt; \n  arrange(symbol, date) |&gt; \n  nest(data = c(date, ret)) |&gt; \n  mutate(arch = map(data, function(.x){\n    \n    ret &lt;- .x |&gt; pull(ret) \n    p &lt;- 2\n    initial_params &lt;- (c(sd(ret), rep(1, p)))\n    fit_manual &lt;- optim(par = initial_params, \n                        fn = logL_arch, \n                        hessian = TRUE, \n                        method=\"L-BFGS-B\", \n                        lower = 0, \n                        p = p,\n                        ret = ret)\n    \n    fitted_params &lt;- (fit_manual$par)\n    se &lt;- sqrt(diag(solve(fit_manual$hessian)))\n    \n    return(tibble(param = c(\"intercept\", paste0(\"alpha_\", 1:p)),\n                  value = fitted_params, se = se))\n  })) |&gt; \n  select(-data) |&gt; \n  unnest(arch) |&gt;\n  summarise(uncond_var = dplyr::first(value) / (1 - sum(value) + dplyr::first(value)))\n  \nunconditional_volatility |&gt; \n  ggplot() + \n  geom_bar(aes(x = reorder(symbol, uncond_var), \n               y = sqrt(250) * uncond_var), stat = \"identity\") + \n  coord_flip() +\n  labs(x = \"Unconditional volatility (annualized)\", \n      y = \"\")"
  },
  {
    "objectID": "exercises/covariance_estimation.html#ledoit-wolf-shrinkage-estimation",
    "href": "exercises/covariance_estimation.html#ledoit-wolf-shrinkage-estimation",
    "title": "(Co)variance estimation",
    "section": "Ledoit-Wolf shrinkage estimation",
    "text": "Ledoit-Wolf shrinkage estimation\nA severe practical issue with the sample variance-covariance matrix in large dimensions (\\(N &gt;&gt;T\\)) is that \\(\\hat\\Sigma\\) is singular. Ledoit and Wolf proposed a series of biased estimators of the variance-covariance matrix \\(\\Sigma\\), which overcome this problem. As a result, it is often advised to perform Ledoit-Wolf-like shrinkage on the variance-covariance matrix before proceeding with portfolio optimization.\nExercises:\n\nRead the introduction of the paper Honey, I shrunk the sample covariance matrix and implement the feasible linear shrinkage estimator. If in doubt, consult Michael Wolf’s homepage. You will find helpful R and Python codes there.\nWrite a simulation that generates iid distributed returns (you can use the random number generator rnorm()) for some dimensions \\(T \\times N\\) and compute the minimum-variance portfolio weights based on the sample variance-covariance matrix and Ledoit-Wolf shrinkage. What is the mean squared deviation from the optimal portfolio (\\(1/N\\))? What can you conclude?\n\nSolutions:\nThe assignment requires you to understand the original paper in depth - it is arguably hard to translate the derivations into working code. However, below are some sample codes you can use for future assignments.\n\ncompute_ledoit_wolf &lt;- function(x) {\n  # Computes Ledoit-Wolf shrinkage covariance estimator\n  # This function generates the Ledoit-Wolf covariance estimator  as proposed in Ledoit, Wolf 2004 (Honey, I shrunk the sample covariance matrix.)\n  # X is a (t x n) matrix of returns\n  t &lt;- nrow(x)\n  n &lt;- ncol(x)\n  x &lt;- apply(x, 2, function(x) if (is.numeric(x)) # demean x\n    x - mean(x) else x)\n  sample &lt;- (1/t) * (t(x) %*% x)\n  var &lt;- diag(sample)\n  sqrtvar &lt;- sqrt(var)\n  rBar &lt;- (sum(sum(sample/(sqrtvar %*% t(sqrtvar)))) - n)/(n * (n - 1))\n  prior &lt;- rBar * sqrtvar %*% t(sqrtvar)\n  diag(prior) &lt;- var\n  y &lt;- x^2\n  phiMat &lt;- t(y) %*% y/t - 2 * (t(x) %*% x) * sample/t + sample^2\n  phi &lt;- sum(phiMat)\n\n  repmat = function(X, m, n) {\n    X &lt;- as.matrix(X)\n    mx = dim(X)[1]\n    nx = dim(X)[2]\n    matrix(t(matrix(X, mx, nx * n)), mx * m, nx * n, byrow = T)\n  }\n\n  term1 &lt;- (t(x^3) %*% x)/t\n  help &lt;- t(x) %*% x/t\n  helpDiag &lt;- diag(help)\n  term2 &lt;- repmat(helpDiag, 1, n) * sample\n  term3 &lt;- help * repmat(var, 1, n)\n  term4 &lt;- repmat(var, 1, n) * sample\n  thetaMat &lt;- term1 - term2 - term3 + term4\n  diag(thetaMat) &lt;- 0\n  rho &lt;- sum(diag(phiMat)) + rBar * sum(sum(((1/sqrtvar) %*% t(sqrtvar)) * thetaMat))\n\n  gamma &lt;- sum(diag(t(sample - prior) %*% (sample - prior)))\n  kappa &lt;- (phi - rho)/gamma\n  shrinkage &lt;- max(0, min(1, kappa/t))\n  if (is.nan(shrinkage))\n    shrinkage &lt;- 1\n  sigma &lt;- shrinkage * prior + (1 - shrinkage) * sample\n  return(sigma)\n}\n\nThe simulation can be conducted as below. First, the individual functions simulate iid normal “returns” matrix for given dimensions T and N. Next, optimal weights are computed using either the variance-covariance matrix or the Ledoit-Wolf shrinkage equivalent. If you want to generate results that can be replicated, use the function set.seed(). It takes any integer as input and ensures that whoever runs this code afterward retrieves the same random numbers within the simulation. Otherwise (or if you change “2023” to an arbitrary other value), there will be some variation in the results because of the random sampling of returns.\n\nset.seed(2023)\n\ngenerate_returns &lt;- function(T, N) matrix(rnorm(T * N), nc = N)\n\n# Minimum variance portfolio weights\ncompute_mvp_sample &lt;- function(mat){\n  w &lt;- solve(cov(mat))%*% rep(1,ncol(mat))\n  return(w/sum(w))\n}\ncompute_mvp_lw &lt;- function(mat){\n  w &lt;- solve(compute_ledoit_wolf(mat))%*% rep(1,ncol(mat))\n  return(w/sum(w))\n}\n\neval_weight &lt;- function(w) length(w)^2 * sum((w - 1/length(w))^2)\n\nNote that I evaluate weights based on the squared deviations from the naive portfolio (the minimum variance portfolio in the case of iid returns). The scaling length(w)^2 is just there to penalize larger dimensions harder.\n\nsimulation &lt;- function(T = 100, N = 70){\n  simulation_rounds &lt;- 50 \n  tmp &lt;- matrix(NA, nc = 2, nr = simulation_rounds)\n  for(i in 1:simulation_rounds){\n    mat &lt;- generate_returns(T, N)\n    w_lw &lt;- compute_mvp_lw(mat)\n    if(N &lt; T) w_sample &lt;- compute_mvp_sample(mat)\n    if(N &gt;= T) w_sample &lt;- rep(NA, N)\n    tmp[i, 1] &lt;- eval_weight(w_lw)\n    tmp[i, 2] &lt;- eval_weight(w_sample)\n  }\n  tibble(model = c(\"Shrinkage\", \"Sample\"), error = colMeans(tmp), N = N, T = T)\n}\n\nresult &lt;- bind_rows(simulation(100, 60),\n                    simulation(100, 70),\n                    simulation(100, 80),\n                    simulation(100, 90),\n                    simulation(100, 100),\n                    simulation(100, 150))\n\nresult |&gt; \n  pivot_wider(names_from = model, values_from = error) |&gt;\n  knitr::kable(digits = 2, \"pipe\")\n\nThe results show a couple of interesting results: First, the sample variance-covariance matrix breaks down if \\(N &gt; T\\) - no unique minimum variance portfolio anymore. On the contrary, Ledoit-Wolf-like shrinkage retains the positive definiteness of the estimator even if \\(N &gt; T\\). Further, the (scaled) mean-squared error of the portfolio weights relative to the theoretically optimal naive portfolio is way smaller for the Shrinkage version. Although the variance-covariance estimator is biased, the variance of the estimator is reduced substantially and thus provides more robust portfolio weights."
  },
  {
    "objectID": "exercises/beta_estimation.html",
    "href": "exercises/beta_estimation.html",
    "title": "Beta Estimation",
    "section": "",
    "text": "Exercises:\n\nRead in the clean CRSP data (crsp_monthly) set from the tidy_finance_*.sqlite file (if you do not recall how to do this, check the previous chapter)\nRead in the Fama-French monthly market returns (factors_ff_monthly) from the database\nCompute the market beta \\(\\beta_\\text{AAPL}\\) of ticker AAPL (permno == 14593). You can use the function lm() (R) or smf.ols (Python) for that purpose (alternatively: compute the well-known OLS estimate \\((X'X)^{-1}X'Y\\) on your own).\nFor monthly data, it is common to compute \\(\\beta_i\\) based on a rolling window of length five years. Implement a rolling procedure that estimates assets market beta each month based on the last 60 observations. You can use the package slider (R), statsmodels.regression.rolling (Python), or a simple for loop. (Note: this is going to be a time-consuming computational task)\nStore the beta estimates in the tidy_finance_*.sqlite database as beta_exercise (the file tidy_finance_*.sqlite already contains a table beta with the estimated values from the textbook - it may be a good idea to compare your results with the ones we get).\nProvide summary statistics for the cross-section of estimated betas\nWhat is the theoretical prediction of CAPM concerning the relationship between market beta and expected returns? What would you expect if you create portfolios based on beta (you create a high- and a low-beta portfolio each month and track the performance over time)? How should the expected returns differ between high and low-beta portfolios?\n\nSolutions: All solutions are provided in the book chapter Beta estimation (R version) or Beta estimation (Python version)",
    "crumbs": [
      "exercises",
      "Asset Pricing",
      "Beta Estimation"
    ]
  },
  {
    "objectID": "exercises/financial_data.html",
    "href": "exercises/financial_data.html",
    "title": "Accessing & managing financial data",
    "section": "",
    "text": "Exercises:\n\nRead the book chapters Accessing & managing financial data and WRDS, CRSP, and Compustat entirely. If you prefer to read Python code, the chapters are available here and here. They may contain some advanced concepts but also a description of almost every important dataset relevant to research in empirical finance.\nConsult the material on Absalon on how to get the raw CRSP data as a KU student. Download the raw monthly CRSP data from the Bloomberg terminals in building 26 and follow the cleaning steps described in Downloading and Preparing CRSP (R and Python). To get more information on how to compute returns adjusted for delisting, follow the procedure described in Chapter 7.2 of the book Empirical Asset Pricing.\nDownload the files tidy_finance_r.sqlite and tidy_finance_python.sqlite from Absalon. Optimally you store it in the folder called data within your standard working directory for the course. Almost all exercises from now on will start with reading data out of this file, so make sure you familiarize yourself with this short minimal setup to load data into your R or Python session memory from a fresh session (you can consult it anytime again later during the course).\n\n\nFrom now on, all you need to do to access data that is stored in the database is to follow three steps: (i) Establish the connection to the SQLite database, (ii) call the table you want to extract, and (iii) collect the data. For your convenience, the following steps show all you need compactly.\n\n\nPythonR\n\n\n\nimport pandas as pd\nimport sqlite3\n\ntidy_finance = sqlite3.connect(\n  database=\"../data/tidy_finance_python.sqlite\"\n)\n\ncrsp_monthly = (pd.read_sql_query(\n  sql=(\"SELECT permno, month, industry, ret_excess \" \n       \"FROM crsp_monthly\"),\n  con=tidy_finance_python,\n  parse_dates={\"month\"})\n  .dropna()\n)\n\n\n\n\n    library(tidyverse)\n    library(RSQLite)\n    tidy_finance &lt;- dbConnect(SQLite(), \n                              \"../data/tidy_finance_r.sqlite\", \n                              extended_types = TRUE)\n    factors_ff_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") \n    factors_ff_monthly &lt;- factors_ff_monthly |&gt; collect()\n\n\n\n\n\nAs always (but this is important): If you need help with the SQL database, post your question on Absalon. Your TA, your peers, and I will help you!\nReplicate the following two figures provided in the lecture slides: i) Create a time series of the number of stocks in the CRSP sample, which are listed on NASDAQ, NYSE, and AMEX. ii) Illustrate the time series of total market values (inflation-adjusted) based on industry classification siccd. The book Empirical Asset Pricing (Bali, Murrey, and Engle) provides a detailed walk-through if you need help.\n\nSolutions: All solutions are provided in the book chapter WRDS, CRSP, and Compustat (R version and Python version) and the lecture slides.",
    "crumbs": [
      "exercises",
      "Asset Pricing",
      "Accessing & managing financial data"
    ]
  },
  {
    "objectID": "exercises/index.html",
    "href": "exercises/index.html",
    "title": "Exercises",
    "section": "",
    "text": "For each week I provide a set of exercises that help you to deepen your understanding of the covered topics. Try to solve the exercises on your own before reading the proposed solutions. Should there be any question, get in touch with your TA, post on Absalon, or discuss with your peers. You can also reach out to me during my office hours.",
    "crumbs": [
      "exercises",
      "Introduction",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/machine_learning.html",
    "href": "exercises/machine_learning.html",
    "title": "Shrinkage Estimation",
    "section": "",
    "text": "In these exercises, you familiarize yourself with the details behind shrinkage regression methods such as Ridge and Lasso. Although R and Python provide amazing interfaces which perform the estimation flawlessly, you are first asked to implement Ridge and Lasso regression estimators from scratch before moving on to using the package glmnet next.",
    "crumbs": [
      "exercises",
      "Machine Learning",
      "Shrinkage Estimation"
    ]
  },
  {
    "objectID": "exercises/machine_learning.html#introduction-to-penalized-regressions",
    "href": "exercises/machine_learning.html#introduction-to-penalized-regressions",
    "title": "Shrinkage Estimation",
    "section": "Introduction to penalized regressions",
    "text": "Introduction to penalized regressions\nExercises:\n\nLoad the file macro_predictors from the tidy_finance_*.sqlitedatabase. Create a variable ythat contains the market excess returns from the Goyal-Welsh dataset (rp_div) and a matrix X that contains the remaining macroeconomic variables except for the column month. You will use X and y to perform penalized regressions. Try to run a simple linear regression of X on `y´ to analyze the relationship between the macroeconomic variables and market returns. Which problems do you encounter?\nWrite a function that requires three inputs, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and lambda and which returns the Ridge estimator (a \\(K\\) vector) for given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should allow you to indicate whether \\(X\\) contains a vector of ones as the first column which should be exempt from the \\(L_2\\) penalty.\nCompute the \\(L_2\\) norm (\\(\\beta'\\beta\\)) for the regression coefficients based on the predictive regression from the previous exercise for a range of \\(\\lambda\\)’s and illustrate the effect of the penalization in a suitable figure.\nNow, write a function that requires three inputs, y (a \\(T\\) vector), X (a \\((T \\times K)\\) matrix), and ’lambda` and which returns the Lasso estimator (a \\(K\\) vector) for given penalization parameter \\(\\lambda\\). Recall that the intercept should not be penalized. Therefore, your function should allow you to indicate whether \\(X\\) contains a vector of ones as the first column which should be exempt from the \\(L_1\\) penalty.\nAfter you are sure you understand what Ridge and Lasso regression are doing, familiarize yourself with the documentation of the package glmnet(). It is a thoroughly tested and well-established package that provides efficient code to compute the penalized regression coefficients not only for Ridge and Lasso but also for combinations, therefore, commonly called elastic nets.\n\nSolutions:\n\nRPython\n\n\n\n# Load required packages\nlibrary(RSQLite)\nlibrary(tidyverse)\n\n# Read in the data\ntidy_finance &lt;- dbConnect(SQLite(),\n  \"../AEF_2024/data/tidy_finance_r.sqlite\",\n  extended_types = TRUE\n)\n\nmacro_predictors &lt;- tbl(tidy_finance, \"macro_predictors\") |&gt;\n  collect()\n\ny &lt;- macro_predictors$rp_div\nX &lt;- macro_predictors |&gt;\n  select(-month, -rp_div) |&gt;\n  as.matrix()\n\n# OLS for Welsh data fails because X is not of full rank\nc(ncol(X), Matrix::rankMatrix(X))\n\n\n\n\nimport pandas as pd\nimport requests\nimport numpy as np\nfrom io import BytesIO, StringIO\nimport zipfile\nimport sqlite3\nfrom plotnine import ggplot, aes, geom_line, labs\nfrom scipy.optimize import minimize\n\n#Read the data\ntidy_finance = sqlite3.connect(database=\"../AEF_2024/data/tidy_finance_python.sqlite\")\n\nmacro_predictors = pd.read_sql_query(\n    sql=\"SELECT * FROM macro_predictors\",\n    con=tidy_finance)\n\n\ny = macro_predictors['dp'].values\nX = macro_predictors.drop(columns=['month', 'dp']).values\n\n# OLS for Welsh data fails because X is not of full rank\nprint(X.shape[1], np.linalg.matrix_rank(X))\n\n\n\n\nAs for OLS, the objective function is to minimize the sum of squared errors \\((y - X\\beta)'(y - X\\beta)\\) under the condition that \\(\\sum\\limits_{j=2}^K \\beta_j \\leq t(\\lambda)\\) if an intercept is present. This can be rewritten as \\(\\beta'A\\beta \\leq t(\\lambda)\\) where \\(A = \\begin{pmatrix}0&&\\ldots&&0\\\\\\vdots& 1&0&\\ldots&0\\\\&\\vdots&\\ddots&&0\\\\0& &\\ldots&&1\\end{pmatrix}\\). Otherwise, the condition is simply that \\(\\beta'I_K\\beta \\leq t(\\lambda)\\) where \\(I_k\\) is an identity matrix of size (\\(k \\times k\\)).\n\nRPython\n\n\n\nridge_regression &lt;- function(y, X, lambda = 0, intercept = FALSE) {\n  K &lt;- ncol(X)\n  A &lt;- diag(K)\n  if (intercept) {\n    A[1, ] &lt;- 0\n  }\n  coeffs &lt;- solve(t(X) %*% X + lambda * A) %*% t(X) %*% y\n  return(coeffs)\n}\n\n\n\n\ndef ridge_regression(y, X, lambda_=0, intercept=False):\n    K = X.shape[1]\n    A = np.eye(K)\n    if intercept:\n        A[0, :] = 0\n    coeffs = np.linalg.solve(X.T @ X + lambda_ * A, X.T @ y)\n    return coeffs\n\n\n\n\nBelow, I apply the function to the data set from the previous exercise to illustrate the output of the function ridge_regression().\n\nRPython\n\n\n\nrc &lt;- c(NA, ridge_regression(y, X, lambda = 1))\n\n# Add an intercept term\nrc_i &lt;- ridge_regression(y, cbind(1, X), lambda = 1, intercept = TRUE)\ncbind(rc, rc_i)\n\n\n\n\nrc = [np.nan] + list(ridge_regression(y, X, lambda_=1))\n\n# Add an intercept term\nX_with_intercept = np.column_stack((np.ones(len(X)), X))\nrc_i = ridge_regression(y, X_with_intercept, lambda_=1, intercept=True)\n\n# Combine rc and rc_i into a single array\nnp.column_stack((rc, rc_i))\n\n\n\n\nFinally, the following code sequence computes the \\(L_2\\) norm of the ridge regression coefficients for a given \\(\\lambda\\). Note that to implement this evaluation in a tidy manner, vectorized functions are important! R provides great ways to vectorize a function, to learn more, Hadley Wickham’s book Advanced R is a highly recommended read!\n\nRPython\n\n\n\nl2_norm &lt;- function(lambda) sum(ridge_regression(y = y, X = X, lambda = lambda)^2) # small helper function to extract the L_2 norm of the ridge regression coefficients\n\nl2_norm &lt;- Vectorize(l2_norm) # To use the function within a `mutate` operation, it needs to be vectorized\n\nseq(from = 0.01, to = 20, by = 0.1) |&gt;\n  as_tibble() |&gt;\n  mutate(norm = l2_norm(value)) |&gt;\n  ggplot(aes(x = value, y = norm)) +\n  geom_line() +\n  labs(x = \"Lambda\", y = \" L2 norm\")\n\n\n\n\ndef l2_norm(lambda_):\n    return np.sum(ridge_regression(y, X, lambda_)**2)\n\nl2_norm_vec = np.vectorize(l2_norm)\n\n\n(pd.DataFrame({'value': np.arange(0.01, 20.1, 0.1)})\n .assign(norm=lambda df: l2_norm_vec(df['value']))\n .assign(gg=lambda df: ggplot(aes(x='value', y='norm'), df))\n .gg[0] + geom_line() + labs(x='Lambda', y='L2 norm')\n)\n\n\n\n\nTo compute the coefficients of linear regression with a penalty on the sum of the absolute value of the regression coefficients, numerical optimization routines are required. Recall, the objective function is to minimize the sum of squared residuals, \\(\\hat\\varepsilon'\\hat\\varepsilon\\) under the constraint that \\(\\sum\\limits_{j=2}^K\\beta_j\\leq t(\\lambda)\\). Make sure you familiarize yourself with the way, numerical optimization in R works: we first define the objective function (objective_lasso()) which has the parameter we aim to optimize as its first argument. The main function lasso_regression() then only calls this helper function.\n\nRPython\n\n\n\nobjective_lasso &lt;- function(beta, y, X, lambda, intercept) {\n  residuals &lt;- y - X %*% beta\n  sse &lt;- sum(residuals^2)\n  penalty &lt;- sum(abs(beta))\n  if (intercept) {\n    penalty &lt;- penalty - abs(beta[1])\n  }\n  return(sse + lambda * penalty)\n}\n\nlasso_regression &lt;- function(y, X, lambda = 0, intercept = FALSE) {\n  K &lt;- ncol(X)\n  beta_init &lt;- rep(0, K)\n  return(optim(par = beta_init, fn = objective_lasso, y = y, X = X, lambda = lambda, intercept = intercept)$par)\n}\n\nrc &lt;- c(NA, lasso_regression(y, X, lambda = 0.01))\n\n# Add an intercept term\nrc_i &lt;- lasso_regression(y, cbind(1, X), lambda = 0.01, intercept = TRUE)\ncbind(rc, rc_i)\n\n\n\n\ndef objective_lasso(beta, y, X, lambda_, intercept):\n    residuals = y - np.dot(X, beta)\n    sse = np.sum(residuals**2)\n    penalty = np.sum(np.abs(beta))\n    if intercept:\n        penalty -= np.abs(beta[0])\n    return sse + lambda_ * penalty\n\ndef lasso_regression(y, X, lambda_=0, intercept=False):\n    K = X.shape[1]\n    beta_init = np.zeros(K)\n    result = minimize(objective_lasso, beta_init, args=(y, X, lambda_, intercept))\n    return result.x\n\n# Without intercept\nrc = np.concatenate(([np.nan], lasso_regression(y, X, lambda_=0.01)))\n\n# With intercept\nrc_i = lasso_regression(y, np.column_stack((np.ones(len(y)), X)), lambda_=0.01, intercept=True)\n\nnp.column_stack((rc, rc_i))\n\n\n\n\nFinally, as in the previous example with Ridge regression, I illustrate how a larger penalization term \\(\\lambda\\) affects the \\(L_1\\) norm of the regression coefficients.\n\nRPython\n\n\n\nl1_norm &lt;- function(lambda) sum(abs(lasso_regression(y = y, X = X, lambda = lambda)))\nl1_norm &lt;- Vectorize(l1_norm) # To use the function within a `mutate` operation, it needs to be vectorized\n\nseq(from = 0, to = 0.5, by = 0.05) |&gt;\n  as_tibble() |&gt;\n  mutate(norm = l1_norm(value)) |&gt;\n  ggplot(aes(x = value, y = norm)) +\n  geom_line() +\n  labs(x = \"Lambda\", y = \" L1 norm\")\n\n\n\n\ndef l1_norm(lambda_val):\n    return np.sum(np.abs(lasso_regression(y=y, X=X, lambda_=lambda_val)))\n\nl1_norm_vec = np.vectorize(l1_norm)\n\n\n(pd.DataFrame({'value': np.arange(0, 0.51, 0.05)})\n .assign(norm=lambda df: l1_norm_vec(df['value']))\n .assign(gg=lambda df: ggplot(aes(x='value', y='norm'), df))\n .gg[0] + geom_line() + labs(x='Lambda', y='L1 norm')\n)\n\n\n\n\nWhile the code above should work, there are well-tested R packages available that provide a much more reliable and faster implementation. Thus, you can safely use the package glmnet. As a first step, the following code create the sequence of regressions coefficients for the Goyal-Welch dataset which should be identical to the example we discussed in the lecture slides.\n\nRPython\n\n\n\nlibrary(glmnet)\n# Lasso and Ridge regression\n\nfit_ridge &lt;- glmnet(X, y, # Model\n  alpha = 0\n)\n\nbroom::tidy(fit_ridge) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  ggplot(aes(x = lambda, y = estimate, color = term)) +\n  geom_line() +\n  geom_hline(data = broom::tidy(fit_ridge) |&gt;\n               filter(lambda == min(lambda)),\n             aes(yintercept = estimate, color = term), linetype = \"dotted\") +\n  theme_minimal() +\n  scale_x_log10() +\n  labs(x = \"Lambda\", y = \"Estimate\")\n\n\n\n\n# import glmnet\n# \n# # Lasso and Ridge regression\n# fit_ridge = glmnet.elnet(X, y, alpha=0)\n# \n# tidy_results = pd.DataFrame({\n#     'term': fit_ridge.get_vnames()[1:],\n#     'lambda': fit_ridge.lambda_,\n#     'estimate': fit_ridge.coef_[1:]\n# })\n# \n# # Filtering out the intercept term\n# tidy_results = tidy_results[tidy_results['term'] != \"(Intercept)\"]\n# \n# # Plotting\n# (ggplot(tidy_results, aes(x='lambda', y='estimate', color='term')) +\n#  geom_line() +\n#  geom_hline(data=tidy_results[tidy_results['lambda'] == min(tidy_results['lambda'])],\n#             aes(yintercept='estimate', color='term'), linetype='dotted') +\n#  theme_minimal() +\n#  scale_x_log10() +\n#  labs(x='Lambda', y='Estimate'))\n\n\n\n\nNote the function argument alpha. glmnet() allows a more flexible combination of \\(L_1\\) and \\(L_2\\) penalization on the regression coefficients. The pure ridge estimation is implemented with alpha = 0, lasso requires alpha = 1.\n\nRPython\n\n\n\nfit_lasso &lt;- glmnet(X, y, # Model\n  alpha = 1\n)\n\nbroom::tidy(fit_lasso) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  ggplot(aes(x = lambda, y = estimate, color = term)) +\n  geom_line() +\n  geom_hline(data = broom::tidy(fit_ridge) |&gt;\n               filter(lambda == min(lambda)),\n             aes(yintercept = estimate, color = term), linetype = \"dotted\") +\n  theme_minimal() +\n  scale_x_log10() +\n  labs(x = \"Lambda\", y = \"Estimate\")\n\n\n\n\n# import glmnet\n# # Lasso and Ridge regression\n# fit_lasso = glmnet.elnet(X, y, alpha=1)\n# \n# # Using broom to tidy up the results\n# tidy_results = pd.DataFrame({\n#     'term': fit_lasso.get_vnames()[1:],\n#     'lambda': fit_lasso.lambda_,\n#     'estimate': fit_lasso.coef_[1:]\n# })\n# \n# # Filtering out the intercept term\n# tidy_results = tidy_results[tidy_results['term'] != \"(Intercept)\"]\n# \n# # Plotting\n# (ggplot(tidy_results, aes(x='lambda', y='estimate', color='term')) +\n#  geom_line() +\n#  geom_hline(data=tidy_results[tidy_results['lambda'] == min(tidy_results['lambda'])],\n#             aes(yintercept='estimate', color='term'), linetype='dotted') +\n#  theme_minimal() +\n#  scale_x_log10() +\n#  labs(x='Lambda', y='Estimate'))\n\n\n\n\nNote in the figure how Lasso discards all variables for high values of \\(\\lambda\\) and then gradually incorporates more predictors. It seems like stock variance is selected first. As expected, for \\(\\lambda \\rightarrow 0\\), the lasso coefficients converge towards the OLS estimates (illustrated with dotted lines).",
    "crumbs": [
      "exercises",
      "Machine Learning",
      "Shrinkage Estimation"
    ]
  },
  {
    "objectID": "exercises/machine_learning.html#factor-selection-via-machine-learning",
    "href": "exercises/machine_learning.html#factor-selection-via-machine-learning",
    "title": "Shrinkage Estimation",
    "section": "Factor selection via machine learning",
    "text": "Factor selection via machine learning\nIn this Chapter, you get to know tidymodels (R) and scikit-learn (Python), a collection of packages for modeling and machine learning (ML) using tidy coding principles. From a finance perspective, you will apply penalized regressions to understand which factors and macroeconomic predictors may help to explain the cross-section of industry returns.\nExercises:\n\nIn this analysis, we use four different data sources. Load the monthly Fama-French 3-factor returns, the monthly q-factor returns from Hou, Xue, and Zhang (2014), the macroeconomic predictors from Welch and Goyal (2008), and monthly portfolio returns from 10 different industries according to the definition from Kenneth French’s homepage as test assets. Your data should contain 22 columns of regressors with 13 macro variables and 8-factor returns for each month.\n\nProvide meaningful summary statistics for the test assets’ excess returns.\n\n\nRPython\n\n\n\nFamiliarize yourself with the tidymodels workflow. First, restrict yourself to just one industry, e.g. Manufacturing. Use the function initial_time_split from the rsample package to split the sample into a training and a test set.\nPreprocess the data by creating a recipe which performs the following steps: First, the aim is to explain the industry excess returns as a function of all predictors. Exclude the column month from the analysis. Include all interaction terms between factors and macroeconomic predictors. Demean and scale each regressor such that the standard deviation is one.\nBuild a model linear_reg with tidymodels with a fixed penalty term that performs lasso regression. Create a workflow that first applies the recipe steps to the training data and then fits the model. Illustrate the predicted industry returns for the in-sample and the out-of-sample period.\nNext, tune the model such that the penalty term is flexibly chosen by cross-validation. For that purpose, update the model such that both, penalty and mixture are flexible tuning variables. Use the function time_series_cv to generate a time series cross-validation sample which allows tuning the model with 20 random samples of length five years with a validation period of four years. Tune the model for a grid of possible penalty and mixture values and visualize the mean-squared prediction errors for the industry returns across the range of possible tuning parameters.\nFinally, write a function to parallelize the entire workflow. The function should split the data into a training and test set, create a cross-validation scheme, and tunes a lasso model for different penalty values. Finally, select the best model in terms of MSPE in the validation test set. Apply the function to every individual industry and illustrate for each industry which factor and macroeconomic variables are selected by the Lasso.\n\n\n\n\nFamiliarize yourself with the scikit-learn workflow. First, restrict yourself to just one industry, e.g. Manufacturing. Use the function train_test_split to split the sample into a training and a test set.\nPreprocess the data with ColumnTransformer which performs the following steps: First, the aim is to explain the industry excess returns as a function of all predictors. Exclude the column month from the analysis. Include all interaction terms between factors and macroeconomic predictors. Demean and scale each regressor such that the standard deviation is one.\nBuild a model ElasticNet with sklearn.linear_model with a fixed penalty term that performs lasso regression. Create a Pipeline that first applies the recipe steps to the training data and then fits the model. Illustrate the predicted industry returns for the in-sample and the out-of-sample period.\nNext, tune the model such that the penalty term is flexibly chosen by cross-validation. For that purpose, update the model such that both, penalty and mixture are flexible tuning variables. Use the function TimeSeriesSplit to generate a time series cross-validation sample which allows tuning the model with 20 random samples of length five years with a validation period of four years. Tune the model for a grid of possible penalty and mixture values and visualize the mean-squared prediction errors for the industry returns across the range of possible tuning parameters.\nFinally, write a function to parallelize the entire workflow. The function should split the data into a training and test set, create a cross-validation scheme, and tunes a lasso model for different penalty values. Finally, select the best model in terms of MSPE in the validation test set. Apply the function to every individual industry and illustrate for each industry which factor and macroeconomic variables are selected by the Lasso.\n\n\n\n\nSolutions: All solutions are provided in the book chapter Factor selection via machine learning (R or Python version).",
    "crumbs": [
      "exercises",
      "Machine Learning",
      "Shrinkage Estimation"
    ]
  },
  {
    "objectID": "exercises/portfolio_choice.html",
    "href": "exercises/portfolio_choice.html",
    "title": "Portfolio Choice under Transaction Costs",
    "section": "",
    "text": "In this exercise, we substantially extend the simple portfolio analysis and bring the simulation closer to a realistic framework. We will penalize turnover, evaluate the out-of-sample performance after transaction costs and introduce some robust optimization procedures in the spirit of the paper Large-scale portfolio allocation under transaction costs and model uncertainty, available in Absalon. We start with standard mean-variance efficient portfolios. Then, we introduce further constraints step-by-step. Numerical constrained optimization is performed by the packages quadprog (for quadratic objective functions such as in typical mean-variance framework) and alabama (for more general, non-linear objectives and constraints). I refer to Exercise Set Portfolio Choice Problems for a basic introduction into numerical optimization, etc.\nExercises:\n\nYou will use the monthly Fama-French industry portfolio returns for the exercise set. Download them directly from Kenneth French’s homepage or extract the data from tidy_finance.sqlite file.\nWrite a function that computes efficient portfolio weight allowing for \\(L_2\\) transaction costs (conditional on the holdings before reallocation). \\(L_2\\) transaction costs mean that within this exercise, we assume that transaction costs are quadratic and of the form \\[\\frac{\\beta}{2}\\left(w_{t+1} - w_{t^+}\\right)'\\left(w_{t+1} - w_{t^+}\\right).\\]. Thus, the function should take the sample estimates \\(\\hat\\mu_t\\) and \\(\\hat\\Sigma_t\\), the previous portfolio weight \\(\\omega_{t^+} := \\frac{\\omega_{t} \\odot \\left(1 + r_t\\right)}{1 + w_t'r_t}\\) where \\(\\odot\\) denotes element-wise multiplication as well as the transaction cost parameter \\(\\beta\\) and the risk aversion \\(\\gamma\\) as inputs. You can consult Equation (7) in Hautsch et al. (2019) for further information. Compute the efficient portfolio for an arbitrary initial portfolio based on the industry returns.\nAnalyse how different transaction cost values \\(\\beta\\) affect portfolio rebalancing. You can assume that the previous allocation was the naive portfolio. Show that for high values of \\(\\beta\\). The initial portfolio becomes more relevant. What happens for different values of the risk aversion \\(\\gamma\\)?\nWrite a script that simulates the performance of 3 different strategies before and after adjusting for transaction costs for different values of \\(\\beta\\) with \\(L_1\\) transaction costs: A (mean-variance) utility maximization (risk aversion \\(\\gamma = 4\\)), a naive allocation that rebalances daily and a (mean-variance) utility with turnover adjustment (risk aversion \\(\\gamma = 4\\)). Assume that \\(\\beta = 1\\). Evaluate the out-of-sample mean, standard deviation, and Sharpe ratios. What do you conclude about the relevance of turnover penalization?\n\nSolutions:\nAll solutions are provided in the book chapter Constrained optimization and backtesting.",
    "crumbs": [
      "exercises",
      "Covariance Estimation and Portfolio Choice",
      "Portfolio Choice under Transaction Costs"
    ]
  },
  {
    "objectID": "exercises/portfolio_choice.html#constrained-optimization-and-backtesting",
    "href": "exercises/portfolio_choice.html#constrained-optimization-and-backtesting",
    "title": "Portfolio Choice under Transaction Costs",
    "section": "",
    "text": "In this exercise, we substantially extend the simple portfolio analysis and bring the simulation closer to a realistic framework. We will penalize turnover, evaluate the out-of-sample performance after transaction costs and introduce some robust optimization procedures in the spirit of the paper Large-scale portfolio allocation under transaction costs and model uncertainty, available in Absalon. We start with standard mean-variance efficient portfolios. Then, we introduce further constraints step-by-step. Numerical constrained optimization is performed by the packages quadprog (for quadratic objective functions such as in typical mean-variance framework) and alabama (for more general, non-linear objectives and constraints). I refer to Exercise Set Portfolio Choice Problems for a basic introduction into numerical optimization, etc.\nExercises:\n\nYou will use the monthly Fama-French industry portfolio returns for the exercise set. Download them directly from Kenneth French’s homepage or extract the data from tidy_finance.sqlite file.\nWrite a function that computes efficient portfolio weight allowing for \\(L_2\\) transaction costs (conditional on the holdings before reallocation). \\(L_2\\) transaction costs mean that within this exercise, we assume that transaction costs are quadratic and of the form \\[\\frac{\\beta}{2}\\left(w_{t+1} - w_{t^+}\\right)'\\left(w_{t+1} - w_{t^+}\\right).\\]. Thus, the function should take the sample estimates \\(\\hat\\mu_t\\) and \\(\\hat\\Sigma_t\\), the previous portfolio weight \\(\\omega_{t^+} := \\frac{\\omega_{t} \\odot \\left(1 + r_t\\right)}{1 + w_t'r_t}\\) where \\(\\odot\\) denotes element-wise multiplication as well as the transaction cost parameter \\(\\beta\\) and the risk aversion \\(\\gamma\\) as inputs. You can consult Equation (7) in Hautsch et al. (2019) for further information. Compute the efficient portfolio for an arbitrary initial portfolio based on the industry returns.\nAnalyse how different transaction cost values \\(\\beta\\) affect portfolio rebalancing. You can assume that the previous allocation was the naive portfolio. Show that for high values of \\(\\beta\\). The initial portfolio becomes more relevant. What happens for different values of the risk aversion \\(\\gamma\\)?\nWrite a script that simulates the performance of 3 different strategies before and after adjusting for transaction costs for different values of \\(\\beta\\) with \\(L_1\\) transaction costs: A (mean-variance) utility maximization (risk aversion \\(\\gamma = 4\\)), a naive allocation that rebalances daily and a (mean-variance) utility with turnover adjustment (risk aversion \\(\\gamma = 4\\)). Assume that \\(\\beta = 1\\). Evaluate the out-of-sample mean, standard deviation, and Sharpe ratios. What do you conclude about the relevance of turnover penalization?\n\nSolutions:\nAll solutions are provided in the book chapter Constrained optimization and backtesting.",
    "crumbs": [
      "exercises",
      "Covariance Estimation and Portfolio Choice",
      "Portfolio Choice under Transaction Costs"
    ]
  },
  {
    "objectID": "exercises/replication-ml-empirical-asset-pricing.html",
    "href": "exercises/replication-ml-empirical-asset-pricing.html",
    "title": "Empirical Asset Pricing via Machine Learning",
    "section": "",
    "text": "This is an attempt to replicate core elements of the paper Empirical Asset Pricing via Machine Learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "In the course “Advanced Empirical Finance” we repeatedly ask: (How) can state-of-the-art methods improve financial decision-making?\nWhile the lecture covers all relevant theoretical aspects and is based on very recent academic papers, you should spend most of your effort on this course on actually doing empirical work. Get your computer ready to work on real problems for financial applications and discuss your code with your peers to acquire the necessary skills to make a difference either in the Finance industry or academia.\nThe exercise sets partially subsume work I created with my colleagues Christoph Scheuch and Patrick Weiss for the books Tidy Finance with R and Tidy Finance with Python. You are very welcome to give us feedback on every aspect of the book such that we can improve the codes, explanations, and general structure. Please contact me or my colleagues directly via contact@tidy-finance.org if you spot typos, mistakes, or other issues that deserve more attention.\nNeedless to say, you should try to solve each question on your own before you refer to my proposed answers. Optimally, you discuss issues with your peers and try to find hints either in the lecture slides or online. There are many ways to get an answer to your questions, most directly you can simply post your questions on StackExchange or Absalon.\nAs an absolute minimum before trying to solve the following exercises, make sure you are familiar with Garrett Grolemund’s and Hadley Wickham’s excellent book R for Data Science. I will make further information or references available on Absalon.\nThings to get done before the first lecture: To dive right into it, check the readme file to comply with the technical prerequisites of this course - you should have R, Python, and Positron ready to get started."
  }
]